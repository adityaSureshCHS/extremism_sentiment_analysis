{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9303bea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/anthony/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the requisite libraries for this notebook\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from scipy.sparse import hstack, vstack, csr_matrix\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Download the VADER lexicon once (comment out after first run if you want)\n",
    "nltk.download(\"vader_lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "463faded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the Dataset\n",
    "\n",
    "df = pd.read_csv(\"extremism_data_final.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03d85a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2776, 3)\n"
     ]
    }
   ],
   "source": [
    "# 2. Clean and Encode Labels\n",
    "\n",
    "# Map EXTREMIST to 1, NON_EXTREMIST to 0\n",
    "label_map = {\n",
    "    \"EXTREMIST\": 1,\n",
    "    \"NON_EXTREMIST\": 0,\n",
    "}\n",
    "\n",
    "# Lambda function to do the encoding\n",
    "def encode_label(textData: str) -> int:\n",
    "    return label_map[textData]\n",
    "\n",
    "# Encoding\n",
    "df[\"Binary_Label\"] = df[\"Extremism_Label\"].apply(encode_label)\n",
    "\n",
    "# Checking the shape\n",
    "print(df.shape)\n",
    "\n",
    "# Our final labels\n",
    "y = df[\"Binary_Label\"].values.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f8549a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (2776, 5286)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, vstack, csr_matrix\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Fit TF-IDF on the whole corpus (up to 4996 features)\n",
    "# -------------------------------------------------------------------\n",
    "MAX_TOTAL_FEATURES = 10000\n",
    "N_VADER_FEATURES = 4\n",
    "MAX_TFIDF_FEATURES = MAX_TOTAL_FEATURES - N_VADER_FEATURES  # 4996\n",
    "\n",
    "texts = df[\"Original_Message\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=MAX_TFIDF_FEATURES,\n",
    "    min_df = 3,\n",
    "    ngram_range = (1, 2)\n",
    ")\n",
    "tfidf_vectorizer.fit(texts)\n",
    "\n",
    "# VADER analyzer (we'll reuse this in the function)\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Define the vectorizer FUNCTION: string -> feature vector\n",
    "# -------------------------------------------------------------------\n",
    "def vectorize_text(text: str):\n",
    "    \"\"\"\n",
    "    Take a single text string and return a feature vector:\n",
    "      [TF-IDF features | VADER neg, neu, pos, compound]\n",
    "\n",
    "    Output shape: (1, n_features) as a sparse CSR matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- TF-IDF part (1 x <=4996) ---\n",
    "    X_tfidf = tfidf_vectorizer.transform([text])  # list of one doc\n",
    "\n",
    "    # --- VADER part (1 x 4) ---\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    vader_vec = np.array([[scores[\"neg\"], scores[\"neu\"], scores[\"pos\"], scores[\"compound\"]]])\n",
    "    X_vader = csr_matrix(vader_vec)\n",
    "\n",
    "    # --- Concatenate horizontally: [TF-IDF | VADER] ---\n",
    "    X_full = hstack([X_tfidf, X_vader], format=\"csr\")\n",
    "\n",
    "    return X_full  # shape: (1, n_features)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Apply vectorize_text() to EACH item in Original_Message\n",
    "# -------------------------------------------------------------------\n",
    "row_vectors = [\n",
    "    vectorize_text(t) for t in df[\"Original_Message\"].fillna(\"\").astype(str)\n",
    "]\n",
    "\n",
    "# Stack all 1-row matrices into a big feature matrix\n",
    "X = vstack(row_vectors)   # shape: (n_samples, n_features)\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)  # (num_rows, <=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc949156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2220, 5286)\n",
      "y_train shape: (2220,)\n",
      "X_val shape:   (556, 5286)\n",
      "y_val shape:   (556,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X: scipy.sparse matrix of shape (N, n_features)\n",
    "# y: numpy array of shape (N,)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,         # 20% validation\n",
    "    random_state=51,\n",
    "    stratify=y             # keep class balance\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_val shape:  \", X_val.shape)\n",
    "print(\"y_val shape:  \", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c44d36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:      0.8004\n",
      "Validation F1 (macro):    0.8003\n",
      "Validation F1 (weighted): 0.8004\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80       285\n",
      "           1       0.79      0.81      0.80       271\n",
      "\n",
      "    accuracy                           0.80       556\n",
      "   macro avg       0.80      0.80      0.80       556\n",
      "weighted avg       0.80      0.80      0.80       556\n",
      "\n",
      "Confusion matrix:\n",
      "[[226  59]\n",
      " [ 52 219]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthony/extremism_sentiment_analysis/.venv/lib/python3.12/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ==========================\n",
    "# MODEL: Linear SVM\n",
    "# ==========================\n",
    "\n",
    "svm_clf = LinearSVC(\n",
    "    C=5.0,                 # you can tune this (0.1, 1, 10, ...)\n",
    "    class_weight='balanced',  # helpful if extremist class is rare\n",
    "    random_state=51\n",
    ")\n",
    "\n",
    "# Train SVM\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_val_pred = svm_clf.predict(X_val)\n",
    "\n",
    "# ==========================\n",
    "# METRICS\n",
    "# ==========================\n",
    "\n",
    "acc = accuracy_score(y_val, y_val_pred)\n",
    "f1_macro = f1_score(y_val, y_val_pred, average=\"macro\")\n",
    "f1_weighted = f1_score(y_val, y_val_pred, average=\"weighted\")\n",
    "\n",
    "print(f\"Validation Accuracy:      {acc:.4f}\")\n",
    "print(f\"Validation F1 (macro):    {f1_macro:.4f}\")\n",
    "print(f\"Validation F1 (weighted): {f1_weighted:.4f}\\n\")\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
